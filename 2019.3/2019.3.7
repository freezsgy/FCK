今天找了一篇论文Recurrent Convolutional Neural Networks for Text Classiﬁcation，2015年的AAAI会议论文

这篇论文将循环神经网络和卷积神经网络结合在一起，进行文本分类。在我们的模型中，我们在学习单词表示的时候尽可能
地使用一个循环结构来捕获上下文信息，这可能会比传统的基于窗口的神经网络引入更少的噪声。我们还使用了一个最大池层
来自动判断哪些单词在文本分类中扮演关键角色，从而捕获文本中的关键组件。

循环神经网络RNN时间复杂度为O(n),该模型通过逐字分析，并将所有的先前文本的语义存储在一个固定大小的隐藏层在那个。
但是，RNN是一个偏倚模型，后面的单词比先前的单词更有优势，因此，当它用于捕获文档的语义时，会降低效率，因为，关
键组键可能出现在文档的任何地方。

这篇论文，引入卷积神经网络CNN，将一个不带偏见的模型引入到NLP任务中，可以很好解决文本中带有最大池化层的识别性短语。
而且CNN模型的复杂度为O(n).并且之前的卷积核大小需要人工尝试，因此，这篇文章解决比传统基于窗口的神经网络学习更多的上下文信息。

这篇论文，引入卷积神经网络CNN，将一个不带偏见的模型引入到NLP任务中，可以很好解决文本中带有最大池化层的识别性短语。而且CNN模型
的复杂度为O(n).并且之前的卷积核大小需要人工尝试，因此，这篇文章解决比传统基于窗口的神经网络学习更多的上下文信息。
