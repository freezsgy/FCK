第6章 信息的度量和作用
1.信息熵
一条信息的信息量与其不确定性有着直接的关系。可以认为，信息量就等于不确定性的多少。用“比特”（Bit）这个概念来度量信息量，信息量的比特数和所有可能情况的对
数函数log有关。
2.信息的作用
信息和消除不确定性是相关联的。一个事物内部会存有随机性，也就是不确定性，假定为U，而从外部消除这个不确定性唯一的方法是引入信息I，而需要引入的信息量取决于
这个不确定性的大小。
3.互信息
当获取的信息和要研究的事物相关性时，这些信息才能帮助我们消除不确定性，最好能够量化地度量“相关性”。
假定有两个随机事件X和Y，它们的互消息定义如下：
I(X;Y)=∑_(x∈X,y∈Y)▒(P(x,y))/(P(x)P(y))，或为I(X;Y)=H(X)-H(X|Y)
所谓两个事件相关性的量化度量，就是在了解了其中一个Y的前提下，对消除另一个X不确定性所提供的信息量。

复习组合数学，准备14号的考试。
