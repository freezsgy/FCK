今日汇报：
今天搜索了一篇关于联邦学习激励机制的文章"A Learning-based Incentive Mechanism for Federated Learning"(2020 IOT)。
在联邦学习中，训练模型被分配给在其本地数据上执行训练任务的参与边缘节点。虽然有数据隐私的好处，但激励机制如何影响参数服务器的效用还不清楚。
本文通过提供基于Stackelberg博弈方法的激励机制来解决这个问题。首先，本文分析了Stackelberg博弈第二阶段纳什均衡的唯一性和第一阶段Stackelberg
均衡的唯一性。其次，基于未共享信息的独特挑战和联合学习中贡献评估的困难，提出了基于drl的激励机制来解决这些问题。最后，通过数值实验进一步
验证了基于DRL的激励机制相对于基线方法的有效性。

斯塔克尔伯格模型（Stackelberg）是一个价格领导模型，厂商之间存在着行动次序的区别。产量的决定依据以下次序：领导性厂商决定一个产量，然后跟随着厂商可以
观察到这个产量，然后根据领导性厂商的产量来决定他自己的产量。需要注意的是，领导性厂商在决定自己的产量的时候，充分了解跟随厂商会如何行动——这意味着领导
性厂商可以知道跟随厂商的反应函数。因此，领导性厂商自然会预期到自己决定的产量对跟随厂商的影响。正是在考虑到这种影响的情况下，领导性厂商所决定的产量将
是一个以跟随厂商的反应函数为约束的利润最大化产量。在斯塔克尔伯格模型中，领导性厂商的决策不再需要自己的反应函数。
