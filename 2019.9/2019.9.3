今日汇报：
看了联邦强化学习的内容。
在强化学习领域中，当状态的特征空间很小、训练数据有限时，构建高质量的策略是很有挑战性的。由于数据和模型的隐私限制，直接从一个智能体迁移数据或者知识到另一
个智能体是不行的。具体来说，作者们假设智能体不会分享它自己的部分观察结果，而且也有一些智能体无法获得反馈；这样的设定就和多智能体强化学习、以及多智能体环
境下的迁移学习都有明显的区别。

在这篇论文中，作者们提出了一种新的强化学习方案，它考虑到了上述的隐私要求，然后在其它智能体的帮助下为每个智能体构建新的 Q 网络。这就是联邦强化学习（FRL）。

联邦强化学习以三个步骤运行。首先，每个智能体都有一个 Q 网络，而且这个 Q 网络的输出是通过高斯差分方法加密保护的，每个智能体也都会收集其他智能体的 Q 网络
输出；然后，所有的智能体会构建一个神经网络，比如多层感知机模型，根据收集的其它智能体的输出和自己的 Q 网络输出计算全局的 Q 网络输出；最后，它会基于全局 
Q 网络的输出同时更新刚才的多层感知机模型和自己的 Q 网络。值得注意的是，多层感知机模型是在所有智能体之间共享的，而智能体自己的 Q 网络对其他智能体都是不
可见的，而且也是无法通过训练过程中共享的那个 Q 网络的加密输出进行推断的。

为了保护数据和模型的隐私，在不同的智能体之间共享信息、更新本地模型时都会对信息使用高斯差分保护。
